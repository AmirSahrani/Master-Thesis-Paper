\newpage
\chapter{Results}
\label{experiment_results}
\lhead{\emph{Experimental Results}}
We first present a full replication and extension of the work by
\citet{radDeliberationSinglePeakednessCoherent2021}. Then we present the simulations based on our model of
meta-deliberation, as well as the results of the sensitivity analysis on both
models. All code for the replication, main experiment and visualizations can be
found in \href{https://github.com/amirsahrani/master_thesis}{this Repository}. \Cref{AppendixB} contains all the values and ranges used for the experiments, as well as supplementary figures.


\section{Replication}\label{sec: replication} We are able to fully replicate the results found by
\citet{radDeliberationSinglePeakednessCoherent2021},  in \Cref{fig:rep_cyclic}
we see that while the bias is less than 0.73, all metric results in a-cyclic
preferences. We also replicate the behavior of the KS metric, where biases in
the range of 0.73-0.85, show even some initial a-cyclic profiles can become
cyclic. \Cref{fig:rep_count} Further explains this by showing that within this
range we always observe 3 unique profile for the KS metric, while DP and CS
have already settled on 6 profiles, thereby representing all possible
preferences. \Cref{fig:rep_condorcet} shows KS introduces ambiguity in the case
that there was a Condorcet winner, resulting in losing the original nice
profile. Finally, the proximity to single-peakedness shows a slightly more
positive note for the KS metric, showing that while the DP and CS bottom out to
the minimum proximity to single-peakedness, KS stays relatively close. Though
this should be taken with a grain of salt, as it is likely a consequence of the
unique preferences being smaller.

\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/cyclic_proportion_Proportion.pdf}
		\caption{The proportion of cyclic profiles remaining, 0 indicating that no cyclic profiles were present after deliberation.}
		\label{fig:rep_cyclic}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\vspace{-9pt}
		\includegraphics[width=\textwidth]{Figures/unique_Unique Preferences.pdf}
		\caption{Number of unique preferences at the final step of deliberation.}
		\label{fig:rep_count}
	\end{minipage}

	\vspace{1em}

	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/condorcet_proportion_Proportion.pdf}
		\caption{The proportion of Condorcet winners left after deliberation, value above one indicate Condorcet winners emerging during deliberation}
		\label{fig:rep_condorcet}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\vspace{-9pt}
		\includegraphics[width=\textwidth]{Figures/sp_proximity_PtS.pdf}
		\caption{Proximity to single-peakedness after deliberation. Proximity to single-peakedness as defined in \Cref{section:related_work}.}
		\label{fig:rep_single_peaked}
	\end{minipage}
\end{figure}



% https://onlinelibrary.wiley.com/doi/10.1002/bimj.202200091
% Things that might end up going into the methods section, but need to be mentioned:
% [X] How is any source of randomness generated
%	If Original groups are used, the only source of randomness is the group that we choose to measure and the voter(s) used to generate the potential candidates
%	If we do not use original groups, then we randomly select voters (uniformly at random)
%	[X] In each case the uncertainty over the candidates is modelled as a normal distribution with mean 0 and std 2
% How are the parameters sampled
%	For sensitivity: Sobol (Side note explain sensitivity analysis)
%	For other experiments: Simply uniform sampling
%	Sample size: Sensitivity chosen such that there are no negative indices, acknowledge that this might result in smaller error bars. Other: We pick 1000, though power analysis would likely be better, limited in number of samples because of the NP-hardness of PtS-V
% [X] What are we trying to estimate
%	We are trying to estimate the final PBS
%	For this we want the smallest average distance between final PBS and simulated PBS
% How are we measuring the performance of the model
%	Absolute difference, since the errors are small there absolute difference and the squared difference are similar.
% How are we going to test this performance
%	We simply look for the best this model can do, and given that we try to explain what factors are important when considering an explanatory model of deliberation.


% Things I need to do: 
% Explain general experimental setup, small recap of what
% we are going to present. Restate the goal of the analysis. How we adapted the
% original data to fit our study, mainly only using voters that have filled in
% both pre and post questionnaires without missing data. Report the number of
% voters before and after this clean up. 

% [X] Show relplots of knowledge and pbs.


\newpage
\section{DeGroot Model}\label{degroot_results}

We present the results of the DeGroot model. For this we take the data from the
\textsc{America in one Room} experiment to inform the support vectors
$\Support$ as well as the estimated support matrices $\EstSupport$.
Specifically, like the original paper, we focus on the most polarizing
questions, the average score on these question is referred to as the
policy-based ideology score (PBS). To clean the data we remove all participants
who are missing at least one question in pre- or post-deliberation
measurements, thus only allowing voter on which we have all information. As a
result, the number of usable opinions is 247 out of 3841 original opinions in
the deliberation group. The support vectors $\Support$ are simply the voters'
reported opinions, where for each topic the voter represents their agreement
with an integer from 0 to 10 (both inclusive). Each voter's estimated support
by each candidate is generated by taking the candidate's true opinion, and
adding random noise to each policy, where the noise is normally distributed
with $\mu = 0$ and $\sigma = 1.37$. The mean of 0 ensures we do not bias the
model preferring candidates with higher or lower average scores, as otherwise
people would consistently be over or underestimating candidates' support. The
standard deviation of 1.37 is chosen to match voter PBS distribution before
deliberation.

To generate a group within which the voter deliberate, we either use the original deliberation groups, in which case we select a group at random and use the voters from that group, note that these are also constrained to voters that have filled in all questions. If instead we generate new groups, we first pick $n$ voters uniformly at random and place them into a single group.

To assess model performance we predict the post deliberation opinion for each
voter as well as for a segment of voters with roughly similar original
opinions. The latter is done by grouping voters in terms of their original
average score into 10 subgroups, we then compare the average opinion of this
group under the simulation vs the opinion in the original data. Though this
does not incorporate \textit{meta-agreement} yet, it allows for the evaluation
of the model without assumptions on how to infer the final ``preferences'' of
the voters. After this assessment, we investigate the convergence of the model,
as well as its sensitivity to the choice of parameters.

Finally, we include meta-agreement into the model and assess the effect on the
final preferences of voters. For this we use the metrics used in the \Cref{sec: replication}.

\subsection{PBS Scores}
We
first proceed with analyzing the performance of the DeGroot model on
substantive agreement. \Cref{fig:pbs} shows the PBS of both the
deliberation and control group, and the simulation results for both instances.
As mentioned in \Cref{sub:americainonroom}, the policy-based ideology score (PBS) is the
average of the 26 most polarizing questions, where a low PBS corresponds to
more liberal answers, and high PBS indicates more conservative answers. As
expected the model performs poorly at predicting the control group, as there
was no significant change for control group members. Similar to the control
group, the starting  PBS of the participants is a strong indicator for
their  PBS. Therefore the model at t=0 is already reasonably aligned with
the final  PBS. We see that after the first time step the PBS scores get
predicted more accurately, after which the model starts making larger
prediction errors. This is because the model keeps averaging all opinions until
a steady state is reaches in which most voters hold non-extreme positions.
Whether this is positive depends on reality, as
\citet{elsterMARKETFORUMThree2002} remarked, if deliberation is able to reach
full consensus, the model might give a glimpse into how this works. If this is
not the case however, then the model is overly naive suggesting that people
come to hold a weighted average of all original opinions. The latter seems more
likely.


\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{Figures/pbs_scores.png}
	\end{center}
	\caption{ PBS, purple indicating the PBS score after deliberation in the original data, green indicates the results of the simulation in that time step.}\label{fig:pbs}
\end{figure}

\Cref{fig:delta_pbs} shows the change in  PBS for the deliberation group,
the original data shows most change happens in people with high  PBS,
getting lower PBS and thus becoming less extreme. The model does not capture
this effect, showing the most change for people with low initial  PBS in
later time steps. This might be because there is a correlation between PBS
score and knowledge. As shown by \citet{fishkinCanDeliberationHave2024}, most
extreme voters, in terms of PBS, seemed to also be the most knowledgeable, if
this was skewed towards voters with high PBS, then these voters would have more
effect on people's opinions in this model when using knowledge-based trust.
Looking at the binned errors in \Cref{fig:binned_errors}, we see that the model
performs better when we do not include knowledge, further indicating that
knowledge is a poor predictor of trust, or persuasiveness. Of course this claim
might be weakened by noting that knowledge in this case is measured by
questions regarding the current state of the America government, such as knowing
which party currently has a majority in the senate. Thus, this specific
knowledge might be insufficient to predict someone's persuasiveness on the
topic of immigration for example.


\begin{figure}[h]
	\begin{center}
		\includegraphics[width=\textwidth]{Figures/change_pbs_scores.png}
	\end{center}
	\caption{Change in  PBS, relative to the original, pre deliberation, measurement. The control is  omitted as there was no significant change.}\label{fig:delta_pbs}
\end{figure}


We note that these slightly positive results appear only when the voters are
grouped by their original PBS, thereby giving the model reasonable predictive
power over a population of voters. We note that this holds even for different
sizes of bins. \Cref{fig:binned_errors} shows the progression of errors over
time when the error is calculated on a per-individual basis, and we find the
model consistently does worse than predicting someone to not change their
opinion, which is what t=0 indicates.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{Figures/errors_binned.png}
	\end{center}
	\caption{Prediction error of the model as a function of time, binned relative to the original  PBS.}\label{fig:binned_errors}
\end{figure}


\begin{figure}[h]
	\centering

	\includegraphics[width=0.6\textwidth]{Figures/bias_time_imshow.png}
	\hspace{1em}
	\caption{PBS Errors as a function of bias and time. Bias acts as a damper: when bias is higher the model take longer to over-estimate the change in opinion.}
	\label{fig:bias_slowdown}
\end{figure}

\Cref{fig:bias_slowdown} shows the relation between the bias factor and the PB
score, showing that the bias does not improve the model's predictive power. As
one might expect a bias is ``slowing down'' the model. Because of this the
model is slower to diverge away from the true opinions.

Finally, \Cref{tab:anova_trust} shows the results of an ANOVA analysis over the
four different methods. The knowledge method is the only non-significant
method, meaning that including knowledge in your model does not significantly
impact the simulation results, and therefore it has no effect on the model
error. All other methods do significantly affect the outcome of the model. We
find that the model has the lowest error when we include ego and similarity,
but exclude self-knowledge. We suspect the reason for ego's positive effect on
the prediction accuracy is similar to that of bias's role in the model. Namely,
ego's is uniquely able to increase the bias a voter has towards themselves,
while self-knowledge can only lower it. As a result the ego is able to benefit
twofold, once by giving voters different biases, which might be more
representative of the actual deliberative process, and by slowing down the rate
of change in a voter's opinion.


\begin{table}
	\caption{F-statistics for the different trust matrix generators.}\label{tab:anova_trust}
	\begin{center}
		\begin{tabular}[c]{lcr}
			\toprule
			Method         & F-statistic & p-value  \\
			\hline
			Knowledge      & 0.3         & 0.57     \\
			Ego            & 48.4        & $<0.01$  \\
			Similarity     & 21.7        & $< 0.01$ \\
			Self-Knowledge & 163.6       & $ <0.01$ \\

			\bottomrule
		\end{tabular}
	\end{center}
\end{table}

\subsection{Convergence}

From \Cref{theory}, we have seen that in the limit some matrices are
convergent, while some are not, in particular if the matrix is aperiodic, it
is convergent. As we model the deliberation group as having fully connected
matrices, with self-loops, the matrices are aperiodic, and thus convergent. We look at the
distance between the estimated support matrix, and the true support matrix, to
get a sense of the rate of convergence. The distance is defined as the
$\ell_1$ norm.

\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{Figures/convergence_groups.png}
	\end{center}
	\caption{Convergence of trust matrices, as measured by the $\ell_1$-norm between the trust matrix at the start and  trust matrix at the current time step.
		t}\label{fig:convergence_big}
\end{figure}

In \Cref{fig:convergence_big}, we see that all configurations converge at a similar rate, reaching their final state around t = 15. As using the original groups leads to generally larger groups we see that that absolute difference in the matrices is smaller. When using knowledge-based trust we see that the distance increases with a zigzag pattern, moving around without converging.

% EXPLAIN why, show longer time scale maybe.




\section{Sensitivity Analysis} We perform sensitivity analysis on the
prediction error of our model on the PBS. For this we do not use the original
groups, instead reassigning voters to random groups of constant size.
\Cref{fig:sensitivty_pbs} shows the sensitivity indices, the \textit{number of voters}
is clearly the biggest factor in the variance of the model. As expected the
\textit{bias} does not contribute to the variance in the model. \textit{Knowledge} informed bias
and\textit{ similarity} informed \textit{bias} both are significantly impacting the variance of
the model. The second order indices show \textit{number of voters} interacts with
self-knowledge and\textit{ similarity}, contributing a large portion of their explained
total variance induced by the \textit{number of voters}. Similarly,\textit{ knowledge}-based
trust and\textit{ similarity}-based trust both have most of their explained variance in
their interactions, since their first order sensitivity is statistically
significant, but low in magnitude.



\begin{figure}[h]
	\begin{center}
		\includegraphics[width=0.95\textwidth]{Figures/senstivity_analysis.png}
	\end{center}
	\caption{First, Second and Total sensitivity indices on the PBS prediction error. The stars in the heat map for the Second order sensitivity indices indicate significant interactions. }\label{fig:sensitivty_pbs}
\end{figure}


\section{Elections}

\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/delib_Mean Number of Cyclic Profiles.png}
		\caption{The proportion of cyclic profiles remaining, 0 indicating that no cyclic profiles were present after deliberation.}
		\label{fig:degroot_cyclic}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\vspace{-9pt}
		\includegraphics[width=\textwidth]{Figures/delib_Mean number of Unique Preferences.png}
		\caption{Number of unique preferences at the final step of deliberation.}
		\label{fig:degroot_count}
	\end{minipage}

	\vspace{1em}

	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Figures/delib_Mean number of Condorcet winners.png}
		\caption{The proportion of Condorcet winners left after deliberation, Values above 1 indicate Condorcet winners emerging during deliberation}
		\label{fig:degroot_condorcet}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\vspace{-9pt}
		\includegraphics[width=\textwidth]{Figures/delib_Mean candidate proximity to single peaked Profiles.png}
		\caption{Proximity to single-peakedness after deliberation, as defined in \Cref{section:related_work}.}
		\label{fig:degroot_single_peaked}
	\end{minipage}
\end{figure}


Firstly, looking at the different voter generation mechanisms, we find that in
general they do not affect the metrics much. The metric for which this is not
true is that of the fraction of elections with a Condorcet winner. Though
slightly unintuitive, we suspect the reason why a single voter's opinion is
more likely to result in a Condorcet winner than the average of 10 voters is
that the true opinions before deliberation were more polarized. As a result,
having multiple ``average'' candidates results in little difference between
them, while an individual voter is more likely to fall close to a large camp of
voters, and thus the alternative representing this voter becomes a Condorcet
winner through being closer to a majority of voters.

Looking at the metrics evaluated on the model, we see similar results as with
the analysis for substantive agreement. At first the simulation starts far from
the metrics from the true score, then it moves towards it and overshoots it
until it starts to converge. Interestingly, for these metrics, the model does require more steps to converge to the same values as the true data.

\textcolor{gray}{This last analysis is very short and needs work}
