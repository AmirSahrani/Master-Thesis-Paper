\chapter{Methods}
\label{Methods}
\lhead{\emph{Methods}} % Set the left side page header to "Symbols"

We proceed with the methods used to replicate the paper by
\citet{radDeliberationSinglePeakednessCoherent2021}, as well as the
experimental setup of our own model. Links to the data used for these
experiments can be found in \Cref{ethics_data}. The programs
are implemented using \texttt{OCaml}, and \texttt{Python}.


\section{Replication} We implement the model as described in
\Cref{section:related_work}. Agents are limited to strict preferences over all
candidates. All experiments are done with 3 alternatives, and 51 voters. The
number of voters is chosen to be an odd number, as this prevents ties between
alternatives. We measure evaluations relating to strict preferences, namely the
proportion of cyclic Profiles, the Number of Unique profiles and the proximity
to single-peakedness by voter deletion (PtS-V), as reported by
\citet{radDeliberationSinglePeakednessCoherent2021}. In addition to those we
also measure the number of Condorcet winners. We do not measure the PtS-C, as
any profile with 2 candidates is single-peaked, thus given the simulation will have 3 candidates
this metric would be of little added value, as all values would be either 1 or $\frac{2}{3}$.

\section{Experiments}

We aim to replicate the findings by the \textsc{America in One room}
experiments \cite{fishkinCanDeliberationHave2024} in-silico. To this end we
use the adaption to the DeGroot model as laid out in \Cref{sec: main model}
The dataset contains a control group as well as an experimental group. In the
dataset, the control group shows no change in opinion over time,
thus this group is best modelled by using the identity matrix $I^n$ as the
trust matrix. The experimental group is modelled as a densely connected
network.  The distribution of the trust we control through 3 methods.

\subsection{Modelling Trust} We propose three different mechanisms through which we
will the trust matrix, as well as the intuitive and theoretical
appeal.

% \textbf{Connectedness}. Firstly, a decision has to be made on whether
% voters in the model are able to communicate with all other voters, if this is the case, we
% get a strongly connected graph, otherwise some voters might not share an edge.
% In the case of small scale deliberation, one might reasonably argue that, at
% least in principle, voters should be able to listen to all other participants,
% and use these opinions to inform their own. In larger settings, such as social
% media platforms or large gathering, it seems that a person's ability
% to communicate effectively with everyone diminishes as the group size grows. In this work, we take
% this to be the main characterization ``true'' deliberation, namely the ability
% to listen to all other participants.

\textbf{Knowledge}. Firstly we consider knowledge, this can be used to
inform both the trust in others, and your bias towards yourself. For this we
can imagine a vector $\boldsymbol{k}$, where each $k_i$ contains some knowledge
score for voter $i$. In modeling, we now have 2 options, firstly, does a voters
knowledge affect their bias towards their own opinion. Intuitively one could
reasonably argue either way. Two plausible ways of reasoning are, ``A
knowledgeable voter knows more facts and is therefore harder to convince'', or
``A Knowledgeable voter realizes the complexity of the topic and is therefore
less certain''. The first line of reasoning seems more general, as it seems
independent of the topic at hand. However, the second line of reasoning seems
to capture something like the Dunning-Kruger effect, which states that people
can have ``meta-ignorance'', meaning they do not realize what they do not realize.

As for the trust a voter places in their peers, a similar argument can be made,
where the voter could either place more trust on people that are more
knowledgeable, and thereby might be able to provide more facts. Or they could
trust less knowledgeable people more because their ignorance allows them to
make more confidant claims, even if these are inaccurate.


\textbf{Similarity}. Finally, voters might simply trust people with
similar opinions more, representing a sort of confirmation bias. This cannot be
applied to ones own opinion in the same sense as with the other two methods.
This could be represented as a sort of ``general bias'', where a voter is more
or less inclined to listen to other opinions.

\textbf{Ego}. Lorem Ipsum.

Given these different options, the right selection of methods becomes question
for empirical observation, which we present in the next Chapter.

Firstly, and most simply, we give all voters a bias. This bias reflects how
much of their trust they place on themselves. For example a bias of 1
represents them placing equal trust on themselves as all other voters combined.
The actual weight on the self loop is calculated as the sum of all incoming
edges multiplied by the bias. Secondly, we have knowledge-based trust, in which
a voter trusts voter $j$ more if voter $j$ is more knowledgeable. We get the
knowledge scores from the \textsc{America in One room} dataset by taking the
proportion of knowledge questions they answered correctly. The interpretation
is that more knowledgeable voters would be more persuasive and thus be more
influential on other voters' opinions. Thirdly, we have credibility-based
trust, where the trust a voter places on another voter is proportional to the
number of outgoing edges that second voter has. This method becomes equivalent
to placing uniform trust in all voters when all voters are situated in a fully
connected graph. If we do not use credibility- or knowledge- based trust, we
call this uniform trust, meaning that they treat all neighbors the same.
Importantly, this does not imply any specific bias value.



\subsection{DeGroot extension}

The first experiments we perform concern the DeGroot model. These experiments
consist of two parts. Firstly we search the parameter space to identify
parameters that best replicate the data, using Bayesian Parameter Estimation.
For this we use data from the \textsc{America in One room} experiment as
described in \Cref{section:related_work}. Though this data does not provide
full preference rankings over the candidates, it does provide data on voters'
opinions on 6 different topics of political discussion, such as climate change
and immigration. Using these opinions, we are able to generate potential
candidates, this is done either by selecting a voter and creating a candidate
with identical opinions, or by pooling 10 voters\footnote{This is arbitrary,
	and it might be good to look into this, but in my opinion this is low priority
	for now. It might also be useful to keep the candidates constant over the
	course of an experiment}
and creating an average of their opinions. Using these
simulated candidates we are able to create preference rankings using the
$\ell_1$-norm. To model the difference between the deliberation and control
group we change the topology of the graphs voters in the respective groups are
situated in. As mentioned before, the deliberation groups will be embedded in a fully connected
graph, while the control groups will be placed on the graph of academic
citations in physics \cite{nr}, this graph is small enough to
allow sampling of the graph for each simulation. Since the original data
provides group numbers for candidates who participated in the deliberation, we
also experiment with replicating these groups as opposed to randomly grouping
voters together.


We measure whether the final profiles are cyclic, whether they have a Condorcet
winner, home many unique profiles there are, and their proximity to being
single-peaked. Proximity to single-peakedness is measured in two ways. When the
simulation size allows for it, we measure the proximity in terms of the number
of voters that would need to be removed for the full profile to become
single-peaked. This particular notion is NP-complete
\cite{erdelyiComputationalAspectsNearly2013}, though it allows for a
2-approximation. We use the method based on an ILP solver, as implemented in
\texttt{PrefTools} \cite{PrefLibPreflibtools2025}. The other notion of proximity is the proximity in
terms of the number of candidates that need to be removed for the profile to
become single-peaked. This can be done in $\mathcal{O}(|V| \cdot{} |C|
	^3)$\cite{przedmojskiAlgorithmsExperimentsNearly}, though the implementation we
use is that of the \texttt{PrefTools} library \cite{PrefLibPreflibtools2025}, which implements
a slower $\mathcal{O}(|V| \cdot{} |C|^5)$ algorithm
\cite{erdelyiComputationalAspectsNearly2013}.




% We aim to find values for all parameters that minimize the error of the model,
% conditional on the number of voters and candidates. For this we define the
% error of our model as the (normalized) difference of the proportion of cyclic
% profiles, the proportion of simulation containing a Condorcet winner, and the
% proximity to single-peakedness through voter deletion, and the
% candidate-based notion. With these parameters, we argue that model captures the
% learning process. We then proceed to analyze convergence behavior under these
% optimal parameters. For this analysis, all configurations are run 100 times.


\renewcommand{\arraystretch}{1.2}
\begin{table}
	\centering
	\begin{tabular}{p{4cm}p{0.65\linewidth }}
		\toprule
		Parameter                     & Description                                                                                                                                      \\
		\midrule
		\texttt{Number of Voters}     & The number of voters in the simulation, representing either the deliberation group, or the control population.                                   \\
		\texttt{Number of Candidates} & The number of candidates to be voted on.                                                                                                         \\
		\texttt{Candidate Generator}  & The way the candidates are generated. Either a random voter is selected for each candidate, or 10 random voters get averaged into one candidate. \\
		\texttt{Bias}                 & The bias all voters have towards their own opinion.                                                                                              \\
		\texttt{Time steps}           & The number of deliberation ``steps'' the voters undergo.                                                                                         \\
		\texttt{Group}                & Use the original groups.                                                                                                                         \\
		\texttt{Similarity}           & Distribute trust based on credibility.                                                                                                           \\
		\texttt{Knowledge}            & Distribute trust based on knowledge.                                                                                                             \\
		\texttt{Ego}                  & Scale voters' bias according to the trust other people have in them                                                                              \\
		\texttt{Self-Knowledge}       & Scale voters' bias according to their knowledge                                                                                                  \\
		\bottomrule
	\end{tabular}
	\caption{The parameters of the DeGroot learning based model, as well as their descriptions}
\end{table}

Given the best configurations, we will analyze the behavior of the model to
understand the convergence on opinion. To this end, we measure the change in the trust
matrix, as well as the distance between each voter's pre- and post-deliberation
preferences using the KS and CS distances. We first aim to find the number of
deliberative steps needed for convergence, which we define as the moment where
the largest change in the trust matrix is smaller than some $\epsilon$. Then we
hope to understand how individual voters' opinions change by looking at the
final state of the trust matrix.

Finally, we use sensitivity analysis to investigate which parameters have the
strongest effect on the model, using Sobol indices to get the first and second
order effects.

