\newpage
\chapter{Theoretical Results}
\label{theory}

\lhead{\emph{Theory}} % Set the left side page header to "Symbols"

In the model of deliberation of \citet{radDeliberationSinglePeakednessCoherent2021}, outlined in \Cref{section:related_work}, aim to model deliberation and show that deliberation results in nicely structured profiles which allow for strategy proof voting rules. One important caveat, given by the authors as well, is all participants should honestly and truthfully participate in deliberation. We now provide a formal statement, showing deliberation does not prevent strategic behavior.

\begin{proposition}
	The process of deliberation over $|\alternatives| \geq 3$ through deterministic deliberation procedure \({D}: \setOfStrictProfiles^n \to \setOfStrictProfiles^n\), followed by voting with voting rule $f$ cannot be surjective, strategyproof and non-dictatorial.

	\label{proposition:deterministic-delib}
\end{proposition}

\begin{proofc}
	Assume, towards a contradiction, such a pair of deliberative procedure ($D$) and voting rule (\(f\)) exists. Any deterministic deliberation procedure $D$ could, in principle, be embedded into a voting rule $f'(\strictProfile) = f(D(\strictProfile))$, such that the voting rule simulates $D$ before applying $f$, which would result in  voting rule $f'$ being surjective, strategyproof and non-dictatorial. This is a contradiction, by the Gibbard-Satterthwaite theorem~\citep{gibbardManipulationVotingSchemes1973,satterthwaiteStrategyproofnessArrowsConditions1975}. \end{proofc}


We extend upon this result, showing the inclusion of biases in voters does not mitigate the negative result. For this we define $\biasedDeliberation$ as follows:

\begin{definition}{Biased Deliberation}{biased_delib}
	A deliberative procedure with biases $\biasedDeliberation : \setOfStrictProfiles ^ n \times \Reals_{[0,1]}^n \to \setOfStrictProfiles^n$ is an extension on a standard deliberative procedure. \(\biasedDeliberation\) has access to the bias each voter has to their own opinion.
\end{definition}

We now proceed with a corollary on \Cref{proposition:deterministic-delib}. Towards this we assume biases are true, in the sense that a voter cannot help but be 'convinced' by the presented profiles as much as their bias allows for this. We think this assumption is a weak and natural in the light of the current model. Furthermore, a violation of this assumption would not imply the following corollary to be false, instead the bias itself becomes a point of strategy, allowing voters to pretend to be more hardheaded than they in fact are.

\begin{corollary}
	A deliberative procedure with biases, followed by voting with any voting rule $f$, cannot be surjective, strategyproof and non-dictatorial

	\label{corollary:biased-delib}
\end{corollary}

The proof of this follows from a reduction of the biased Deliberation $\biasedDeliberation$ to general deliberation $D$.

\begin{proofc}{}
	Take any election consisting of biased deliberation $\biasedDeliberation$ and voting rule $f$, since biases $\boldsymbol{b}$ are true by assumption, they must be fixed, meaning that $\boldsymbol{b}$ is not reported but some fact of the matter. If this election was immune to strategic manipulation, then a deliberative procedure $D$ could embed this $b$, and simulate biased deliberation $\biasedDeliberation$, resulting in $D'(\strictProfile) = \biasedDeliberation(\strictProfile, \boldsymbol{b})$. As a direct corollary to \Cref{proposition:deterministic-delib}, such a $D'$ cannot be surjective, strategyproof and non-dictatorial, showing a contradiction.
\end{proofc}

This result is independent of the metric space chosen. From here we now show that even if we take the deliberation procedures on its own, it still not immune to strategic manipulation. For this we restate strategyproofness as follows:

\begin{definition}{Strategyproofness of Deliberation}{delib-strategyproof}
	A deliberation procedure is strategyproof if there exists no voter $i$ such that there is a profile $\strictProfile$, in which $i$ misreporting their preference $\Preference_{i}$ as $\Preference_{i}'$ results in the profile after deliberation $D(\strictProfile)$ is further from the $i$'s original preference than if they had reported $\Preference_{i}'$. This distance is measured as
	\[\operatorname{Dist}(\Preference_{i}, D(\strictProfile)) \geq \operatorname{Dist}(\Preference_{i}, D(\strictProfile')).\]
	Where the Dist function is simply the sum of all distances between $\Preference_i$ and all preferences in $\strictProfile$.
\end{definition}

One important note is that in the final profile, the preferences of voter \(i\) might not be the same as it was before the deliberation. That is why the distance is calculated w.r.t. \(i\)'s original preference. Intuitively this could be read as \(i\) misreporting their preference to prevent even their own mind from being changed.
Using this definition, we show that the deliberative procedures, under the metric spaces \emph{KS}, \emph{ DP}, \emph{ CS} are not strategyproof. Stated as follows:

\begin{proposition}
	Deliberation under distance measures \emph{KS}, \emph{ DP}, \emph{ CS} is not strategyproof, for $n \geq 2$ and $m \geq 3$.
\end{proposition}
We provide a proof by construction, we show how to do this for KS and DP, as they share the same profiles for this proof. The proof for CS is laid out in \Cref{AppendixA}
\begin{proofc}
	Assume the following population: we have voter 1 whose bias is $1$, and all other voters $j \neq 1$ have bias $0.5$. Furthermore, we have $\operatorname{Dist}(\Preference_1, \Preference_j) = 2$ for all $j$. Voter $1$ now has the option to report $\Preference_{1}'$ instead, which has $\operatorname{Dist}(\Preference_{1}', \Preference_j) = 4$ and $\operatorname{Dist}(\Preference_{1}', \Preference_1) = 2$. If voter $1$ reports $\Preference_1'$, then all $j$ will update towards $1$'s true preference, as using \cref{eq:deliberation_step_formula} we get $r(R_{j}, R_{1}', R_1) = 4$, while $r(R_{j}, R_{1}', R_j) = r(R_{j}, R_{1}', R_1') = 16$.

	Resulting in $\operatorname{Dist}(R_{1}, D(R_{1},\strictProfile_{-1})) = 2(n-1) >  \operatorname{Dist}(R_{1}, D(R_{1}', \strictProfile_{-1})) = 0$.

	Since 1 has a bias of 1, the order of the deliberation has no effect.

	We now show that for distance measures KS and DP, there exists these 3 preference orderings such that the necessary profile can be constructed. We use the following profiles:
	\[
		\begin{aligned}
			R_{1}' & = a \pref c \pref b \pref \cdots \pref m, \\
			R_{1}  & = a \pref b \pref c \pref \cdots \pref m, \\
			R_{j}  & = b \pref a \pref c \pref \cdots \pref m. \\
		\end{aligned}
	\]

	As we are only allowing strict preferences, both distance metrics behave the same locally, with the distance of two profiles being 2 whenever one is 1 swap of alternatives away from the other. This means that  \(R_{i}\) and \(R_{j}\) have a distance of 2, as well as  \(R_{1}'\) and \(R_{1}\) having a distance of 2. In this case the total distance from \(R_{1}'\) to \(R_{j}\) is simply the sum of the local distances for both distance metrics, thus satisfying our requirements.
\end{proofc}


These results show it is likely frivolous to attempt to design a strategy proof deliberation procedure of the likes shown. Instead, focus is now brought to modeling `ideal' deliberation, as laid out in \Cref{subsection:Meta-agreement}. We provide the following mathematical formulations to the four tenants laid out. \emph{Freedom}: voters can report any preference, \emph{Reason}: voters are rational, \emph{Equality}: no voter has special rights \emph{Consensus}:  voters deliberate aim to reach consensus. Which we extend with \emph{Honesty}: Voters represent their true beliefs and preferences only.


\section{Our model}
\label{sec: main model}

In an attempt to model meta-agreement through deliberation, our model needs to make a proper distinction between the `substantive level' and the `meta level'. In order to do so, we propose the following, let \(\policies = \{\policy_{1}, \cdots \policy_{k}\}\) denote the set of policies that could be implemented, where each $\policy_i \in \Reals^{m}$. A voter $i \in \voters$, at the base level has support for these policies, represented as an integer on an interval over $\Reals$. At a meta level, however, a voter has an understanding of which policies are supported by which alternatives. This is modelled as matrix, representing the estimated support for each policy for a candidate, thus voter $i$ has $\EstSupport_i(\policy_j, x)$, which returns these voters' estimated support of $\policy_j$ by alternative $x$.

This model does not explicitly model $D1$, the discovery of a common issue dimension, on the one hand, if the alternatives can be reduced to a line, this model should be able to capture this, even if this one line crosses through multiple issue dimension. For example if all issues are strongly (negatively) correlated on the side of the alternatives, but not on the voters, this model allows for the voters to recognize this by properly estimating the alternatives' support matrices, while voters themselves can keep an uncorrelated support vector. In the case that the actual issue dimension is simply not included in $\policies$, our model would not be able to discover this new dimension, even if human deliberation feasibly could. More straightforwardly, if we the measured support is irrelevant to the true issue dimension(s), our model cannot recover the true issue dimension.

More specifically, our model is adapted from the DeGroot learning model, which originally strictly models probability distributions. In that model, a voter is a node in a graph, and deliberation can be modeled as a Markov chain. In our model, we keep voters as nodes on a graph, as well as a Markov chain, however, instead of a probability distribution, a voter has a support vector $S_i \in \Reals^{|\policies|}_{[0,1]}$, and estimated support matrix $\EstSupport_i\in \Reals^{|\alternatives| \times |\policies|}_{[0,1]}$.

Note that this does not mean that all policies have to have any (estimated) support, nor that an alternative can only support a specific number of policies, in principle there can be alternatives that represent the status quo, and thus do not support any policies, and there can be alternatives that are estimated to support all policies.  Let $\boldsymbol{S}= [\Support_1, \dots, \Support_{n}]^{T}$ denote the population opinion, which has shape \(|\voters| \times |\policies|\).

In order to extract a ballot from this matrix, we assume a voter ranks the alternatives such that the most preferred alternative has the smallest distance between the estimated support matrix for that alternative and her own. We further allow this distance to be weighted, such that a voter may have one or more policies their think are more important. We normalize these weights such that the sum of weights is 1.

Next we define the deliberative procedure, for which we will provide two models. Firstly a simpler model, which stays closer to the original DeGroot model, using only a transition matrix. Secondly we extend this model to be an Agent Based model, in which we allow agents to have additional properties. In both models we allow voters to deliberate on both their own support vector and the estimated support matrix, capturing deliberation on substantive basis as well as a meta basis, respectively.

Firstly, a deliberative step can be modelled using a transition matrix $T$, defined as follows:
\[
	T=\begin{bmatrix}
		t_{11} & \dots  & t_{1n} \\
		\vdots & \ddots & \vdots \\
		t_{n1} & \dots  & t_{nn} \\
	\end{bmatrix}
\]

Here each $t_{ij}$ represents how much voter $i$ trusts the opinion of voter $j$, in order for this to be a proper stochastic matrix, all rows must sum to one, and have non-negative entries. Although this last requirement could be seen as unrealistic, as a voter might actively distrust another voter and update away from their opinion.

Using this, we can now model the opinions of voters after a deliberative step as a matrix multiplication on some matrix $M$:

\begin{equation}
	M^{(1)} = TM^{(0)}
	\label{eq:update_degroot}
\end{equation}

Each entry in the matrix then is simply a linear combination of the other entries in that same column in $M^{(0)}$. In the case of $M = \EstSupport$, this means that voter $i$'s support vector becomes a linear combination of all support matrices, weighted by the trust in each voter. Deliberation can now be modelled by taking powers of the trust matrix, $T^{t}$, representing $t$ deliberation steps. This matrix now represents how much each voter $i$ has learned from the other voters, and can then be used to right multiply both the support and the estimated support matrix to calculate a voters beliefs after deliberation.

Finally, we provide an example of the first deliberation round in example \ref{example:deGroot-delib}, since it is identical for both $\Support$ and  $\EstSupport$, we only show it for $\EstSupport$. The example also shows how voters can initially agree on their support for policies, while disagreeing on their preferred candidates, using meta-agreement to come to a consensus.

\begin{example}{DeGroot deliberation}
	{}

	We have voters \(\voters = \{1,2\}\), events \(\policies = \{\policy_{1}, \policy_{2}\}\), and candidates \(\alternatives = \{a,b\}\). The voters both think that \(\policy_{1} = 1, \policy_{2} = 0\), meaning that they fully support the first policy and reject the second, they estimate the support by alternatives as:
	\[
		\begin{array}{c|cc}
			1 & \policy_1 & \policy_2 \\
			\hline
			a & 0.5       & 0         \\
			b & 0.5       & 1         \\
		\end{array}
		\hspace{1em}
		\begin{array}{c|cc}
			2 & \policy_1 & \policy_2 \\
			\hline
			a & 1         & 0.9       \\
			b & 1         & 0.1       \\
		\end{array}
	\]
	We can encode this into the estimated support matrices as follows:

	\[
		\EstSupport_1 =\begin{bmatrix}
			0.5 & 0 \\
			0.5 & 1 \\
		\end{bmatrix}\quad
		\EstSupport_2 =\begin{bmatrix}
			1 & 0.9 \\
			1 & 0.1 \\
		\end{bmatrix}
	\]

	This results in voter 1 preferring candidate $b$ over candidate $a$, while voter 2, prefers $a$. Intuitively, since voter 1 thinks $\policy_{1}$ is equally supported by each alternative, while $\policy_{2}$ is not supported by $a$, it makes sense for them to prefer candidate $a$. Looking at the distances, we see that the absolute distance between voter 1 and alternative $a$ is 0.5, while for alternative $b$ it is 1.5. For voter 2 we see that the distance to $a$ is 0.9, while for alternative $b$ is it 0.1. Thus, voter 2 prefers $b$ to $a$.


	Now deliberating with the following trust matrix:
	\[
		T=\begin{bmatrix}
			0.3	0.7
			0.2	0.8
		\end{bmatrix}
	\]
	We get the following updated opinions:
	\begin{align*}
		\boldsymbol{\boldsymbol{\EstSupport}}^{(1)} & = T\boldsymbol{\boldsymbol{\EstSupport}}^{(0)}                                                                     \\
		                                            & =T\begin{bmatrix}\EstSupport_1 \EstSupport_2\end{bmatrix}^{T}                                                      \\
		                                            & =\begin{bmatrix}(0.3\EstSupport_1 + 0.7\EstSupport_{2}) & (0.2\EstSupport_1 + 0.8\EstSupport_{2})\end{bmatrix}^{T} \\
		                                            & = \begin{bmatrix}
			                                                \begin{bmatrix}
				0.85 & 0.63 \\
				0.85 & 0.37 \\
			\end{bmatrix} &
			                                                \begin{bmatrix}
				0.9 & 0.72 \\
				0.9 & 0.18 \\
			\end{bmatrix}
		                                                \end{bmatrix}^{T}
	\end{align*}

	These new estimates are not yet in full consensus, however, looking at their corresponding ballots there is consensus on their most preferred alternative, as they both agree that alternatives support $\policy_1$ equally, while $b$ supports $\policy_2$ less.

	\label{example:deGroot-delib}
\end{example}

\subsection{Consensus}
\label{sub: concensus DeGroot}
\textcolor{RedViolet}{In using a single trust matrix for both support and estimated support matrices, we either have both meta and substantive agreement, or neither. But using two matrices would require justification, Maybe here we can introduce biases again, were we assume the original trust matrix, but we add some kind of "meta" bias and "substantive" bias, where these biases reflect how much they are willing to change their minds. In terms of analysis this would not really make things different, but for experimentation we might be able to argue that people might be more willing to change their views on the meta level, and we can experiment with lower substantive bias.}

Using this model of deliberation, meta-agreement can be seen as some estimated support matrix over all policies. If the goal of deliberation is meta-agreement, then the study of interest becomes the dynamics of convergence towards a unified estimate.

We present a summary of results relating to strongly connected graphs, as well as graphs for which there exists only closed and strongly connected subsets of nodes. For other results we refer to \citet{golubNaiveLearningSocial2010}. Firstly we focus on the strongly connected graphs.

\begin{proposition}{(\citet{golubNaiveLearningSocial2010}).}
	For a strongly connected matrix \(T\), the following properties are equivalent:
	\begin{itemize}
		\item[o] \(T\) is Convergent
		\item[o] \(T\) is Aperiodic
		\item[o] There exists a left eigenvector \(\boldsymbol{s}\) for matrix \(T\), with corresponding eigenvalue \(1\), whose entries sum to one, such that for every $P_i$, we have
			\[\left(\lim_{t\to \infty}T^{t} \boldsymbol{P}\right)_{i} = \boldsymbol{s}\boldsymbol{P}\]
	\end{itemize}
\end{proposition}

This result is positive for studying the convergence dynamics, as no knowledge of the initial distribution is needed to determine convergence, it allows us to simply verify one of these three properties on the network. Though strongly connected graphs might be a strong requirement, in the case of small scale (in person) deliberation, this might be realistic. Fortunately, even outside this setting it might be possible to reach convergence. For this we first define what a closed set of nodes is.

\begin{definition}{Closed set of Nodes}{}
	A set of Nodes \(C = \{1, \dots, n\}\) is closed if for each \(i,j \in C\) we have $T_{ij} \geq 0$ and for each \(i \in C, j \notin C\) we have \(T_{ij} = 0\)
\end{definition}

Using this definition, if each node is part of a closed set, we can form the following proposition

\begin{proposition}{(\citet{golubNaiveLearningSocial2010}).}
	If for each \(i \in N\), \(i\) is a member of a closed set in the graph, and each closed set is strongly connected, \(T\) is convergent.
\end{proposition}

\section{Agent Based Deliberation}
\textcolor{RedViolet}{I have not thought about the following section much yet, I think in principle the elements in orange might be interesting, but I could also easily see them being too complicated to implement while also being not very useful. Everything else I have some more confidence in.}

For the Agent Based model, we allow for granularity in how voters update their support, as well as evolving the trust matrices. Firstly, each voter now has a knowledge score, \textcolor{orange}{an importance vector, and a history}. These additional properties allow for the following, firstly with a knowledge score, we can inform the transaction of ideas between two voters, for example, more knowledgable voters could be more convincing. Secondly using the importance vector, we can guide conversation, two voters deliberating might place emphasis on discussing policies they deem most important, and might be more open to changing their minds on less important policies. Finally using the history, the trust matrix can be updated to allow voters to trust people they have successfully interacted with more. We will now define how these interactions occur based on the trust matrix.


During each time step $t$, voter $i$ picks a conversational partner, where the probability of talking to some voter $j$ is determined by the $T_{ij}$. Once each voter has picked their conversational partner, groups are formed to contain all voters in a chain of conversation, for example if voter 1 decided to speak to voter 2 and voters 2 and 3 decide to talk to each other, they end up talking between the three of them. It is important to note that a voter can "talk to themselves", which results in them not talking at all unless it so happens that someone speaks to them. All voters now update their history to include their current conversational partners. During the conversation, the first step is to determine whether to discuss on the meta or the substantive level, randomly decided, where the probabilities are proportional \textcolor{red}{T.B.D.}.
Then, each voter announces their (estimated) support on a \textcolor{orange}{subset} of the policies, this subset is determined by her importance vector, where she announces her support on that topic with probability proportional to the importance. As a result all voters update their views, based on their knowledge, the knowledge of the announcer, the voter's importance placed on the policy and finally the trust they have in the opinion of the announcer. We capture by minimizing the following formula, adapted from \citet{radDeliberationSinglePeakednessCoherent2021}:

\begin{equation}
	\label{eq:abm_update_function}
	% \sqrt{k_{i}\iota_{i,j} b_{i}d(\Support{i,j}, \Support{i,j}')^{2} - ((1-k)(1-\iota_{i,j})(1-b_{i})d(\Support{i,j}, \Support{i,j}')^{2})}
	\sqrt{
		(1-(k_j-k_i))b_id(\Support_i, \Support')^{\iota_i + \iota_j}
		-
		(1 - (k_i-k_j))(1-b_i)d(\Support_j, \Support')^{\iota_j + \iota_i}
	}
\end{equation}

Where $k_{i}$ is voter $i$'s knowledge, normalized to be between 0 and 1, $\iota_{i,j}$ is the importance she places on policy $\policy_{j}$, and $b_{i}$ is the biases she has towards her own opinion.

\textcolor{orange}{The following updates seem useful to allow us to capture how voters don't just change their opinion, but their general attitudes towards each other and the policies as well. The aim to use this to allow us to capture the following:}
\textcolor{orange}{
	\begin{itemize}
		\item Voters becoming more knowledgable (This was observed in the America in one room experiment)
		\item Voters grouping together, essentially creating "friendships" with people they talked to before. I might want to change the model to also allow people do trust people less under some circumstances
		\item Change the importance the place on policies as a function of the "social group"
	\end{itemize}}

\textcolor{orange}{
	Furthermore, I would use this to argue why small scale deliberation with strangers is as beneficial as it is, since people do not know each other, all these are "uniform", and they get to interact more sincerely. This apposed to broad interaction people have in their daily lives where they interact with similar people, and place importance accordingly.
}

After the conversation step, a voter can update her knowledge, importance and trust. Firstly a voter's knowledge increases as \textcolor{red}{T.B.D.}, the importance on each topic increases, and is renormalized. Finally a voter's trust in their conversational partners increases with some amount, distributed over all the conversational partners in their group at time $t$, and is again renormalized. This trust updating allows us to capture two things: People can get into (small) echo chambers, where a voter can start to distrust everyone but themselves (or a small selection of people). This however, also lets us model exposure effects, where a voter can come to trust people after their first interaction with them.

\textcolor{orange}{
	I think that we can use the agent based framework to get some simple results that make intuition more formal. For example as I just mentioned, echo chambers can be a result of consecutive isolation of small groups or individuals. I did not write any of these out as the specifics will strongly depend on the updating functions as well as which parameters we end up including.
}
